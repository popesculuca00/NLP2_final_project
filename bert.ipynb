{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import transformers\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    \n",
    "from utils import read_process_data\n",
    "\n",
    "\n",
    "def additional_preprocessing(df):\n",
    "\n",
    "    columns_map = { \"text\": \"body\", \"sentiment\": \"emotion\"}\n",
    "    df = df.drop(columns = [column for column in df.columns if column not in columns_map.keys()])\n",
    "    df.columns = df.columns.map(columns_map)\n",
    "\n",
    "    df = df.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = \"data/train.csv\"\n",
    "TEST_DATA_PATH = \"data/test.csv\"\n",
    "\n",
    "train_data = read_process_data(TRAIN_DATA_PATH)\n",
    "test_data = read_process_data(TEST_DATA_PATH)\n",
    "\n",
    "train_data = additional_preprocessing(train_data) \n",
    "test_data = additional_preprocessing(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "bert_model = RobertaModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1773, 0.8239, 1.0674], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight(class_weight=\"balanced\", \n",
    "                                     classes= [\"negative\", \"neutral\", \"positive\"], \n",
    "                                     y=train_data[\"emotion\"])\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data.to_dict()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_encoder = {\n",
    "            \"negative\": 0,\n",
    "            \"neutral\": 1,\n",
    "            \"positive\": 2\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"body\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[\"body\"][idx], self.label_encoder[self.data[\"emotion\"][idx]]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'targets': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Tokenize and prepare the dataset\n",
    "max_len = 128  # or another appropriate length\n",
    "train_dataset = SentimentDataset(train_data, tokenizer, max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=0, shuffle=True)\n",
    "\n",
    "\n",
    "valid_dataset = SentimentDataset(test_data, tokenizer, max_len)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, num_workers=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "\n",
    "        for param in list(self.bert.parameters())[:-10]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.LeakyReLU(0.01)\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        \n",
    "        first_layer = self.linear(dropout_output)\n",
    "        return self.classifier(first_layer)\n",
    "\n",
    "model = SentimentClassifier(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(preds, y):\n",
    "    max_preds = preds.argmax(dim=1)  # get the index of the max probability for each sample\n",
    "    correct = max_preds.eq(y)\n",
    "    return correct.sum().cpu() / torch.FloatTensor([y.shape[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "total_steps = len(train_loader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = \"roberta-finetune-logs-full-unfrozen\"\n",
    "logger = TensorBoardLogger(RUN_ID)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"results/{RUN_ID}/checkpoints\",\n",
    "    filename=\"model-{epoch:02d}-{valid_loss:.5f}\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"valid_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentimentClassifier(pl.LightningModule):\n",
    "    def __init__(self, model=model, criterion=criterion, class_weights=class_weights):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def calculate_accuracy(self, preds, y):\n",
    "        max_preds = preds.argmax(dim=1)  # get the index of the max probability for each sample\n",
    "        correct = max_preds.eq(y)\n",
    "        return correct.sum().cpu() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        return self.model(ids, mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ids, mask, targets = batch['ids'], batch['mask'], batch['targets']\n",
    "        outputs = self(ids, mask)\n",
    "        loss = self.criterion(outputs, targets.long())\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ids, mask, targets = batch['ids'], batch['mask'], batch['targets']\n",
    "        outputs = self(ids, mask)\n",
    "        loss = self.criterion(outputs, targets.long())\n",
    "        acc = self.calculate_accuracy(outputs, targets)\n",
    "        self.log('valid_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('valid_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        # print(f\"Valid {loss=}  {acc=}\")\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizers_dict = {}\n",
    "        optimizer = AdamW(self.model.parameters(), lr=0.0005)\n",
    "        optimizers_dict[\"optimizer\"] = optimizer\n",
    "        \n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=2, min_lr=0.000005)\n",
    "        optimizers_dict[\"lr_scheduler\"] = scheduler\n",
    "        optimizers_dict[\"monitor\"] = \"valid_loss\"\n",
    "        return optimizers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "sentiment_classifier = SentimentClassifier(model, criterion, class_weights)\n",
    "\n",
    "# Configure Trainer\n",
    "trainer = Trainer(max_epochs=epochs,\n",
    "                  logger=logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: roberta-finetune-logs-full-unfrozen\\lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | model     | SentimentClassifier | 124 M \n",
      "1 | criterion | CrossEntropyLoss    | 0     \n",
      "--------------------------------------------------\n",
      "5.5 M     Trainable params\n",
      "119 M     Non-trainable params\n",
      "124 M     Total params\n",
      "499.373   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfc99b1236a44b5845ae62af2748d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "d:\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870a5fffbd04452b8afa0d1e0ce1c945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7f12568e2c45159c37f26bc72ad04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2482fb61864c4d35962cac350c49ab63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42a7cded3f340648914f64d91d228e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327274dcd3c2442db569a42172b74a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e1481fbd654c8a8481dcd0987d01f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a6c0dab55f4776bbe399ec5dda728f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921914338a504b88bec89d2772098f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbf45a7026d4a18a04d19b9471da8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a9fb74a5a9433e89149955e047c8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751ebaf447e74409982726d7fad002ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b34a49689f4d1dacefd3aae13fae5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d672c3bd474ce88eedafb411d599b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# trainer.fit(sentiment_classifier, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = SentimentClassifier.load_from_checkpoint(checkpoint_path=\"results/roberta-finetune-logs-full-unfrozen/lightning_logs/version_0/checkpoints/epoch=11-step=10308.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds, act = [], []\n",
    "    for batch in valid_loader:\n",
    "        ids, mask, targets = batch['ids'], batch['mask'], batch['targets']\n",
    "        cnt_preds = model_trained(ids.to(device), mask.to(device))\n",
    "        preds.append(cnt_preds.argmax(dim=1))\n",
    "        act.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.634691596031189\n"
     ]
    }
   ],
   "source": [
    "preds_cat = torch.cat(preds, dim=0).cpu()\n",
    "act_cat = torch.cat(act, dim=0).cpu()\n",
    "acc = preds_cat.eq(act_cat).sum() / preds_cat.shape[0]\n",
    "\n",
    "print(f\"Final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
